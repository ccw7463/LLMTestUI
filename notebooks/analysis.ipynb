{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# 모델의 이름 또는 경로를 사용하여 config 정보만 가져오기\n",
    "qwen_config = AutoConfig.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "llama_config = AutoConfig.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "gemma_config = AutoConfig.from_pretrained(\"google/gemma-2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a short introduction to large language model\"}\n",
    "]\n",
    "prompt = qwen_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a bot that responds to weather queries.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hey, what's the temperature in Paris right now?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_current_temperature(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "    return 22.  # A real function should probably actually get the temperature!\n",
    "\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n",
    "]\n",
    "\n",
    "prompt = llama_tokenizer.apply_chat_template(messages, \n",
    "                                             tokenize=False,\n",
    "                                             #tools=[get_current_temperature], \n",
    "                                             add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write a hello world program<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = [{ \"role\": \"user\", \"content\": \"Write a hello world program\" },]\n",
    "prompt = gemma_tokenizer.apply_chat_template(chat, \n",
    "                                            tokenize=False, \n",
    "                                            add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_sequences = []\n",
    "if gemma_tokenizer.additional_special_tokens:\n",
    "    for stop_seq in gemma_tokenizer.additional_special_tokens:\n",
    "        if \"end\" in stop_seq:\n",
    "            stop_sequences.append(stop_seq)\n",
    "stop_sequences.append(gemma_tokenizer.eos_token)\n",
    "final_stop_sequences = []\n",
    "for item in stop_sequences:\n",
    "    if item not in final_stop_sequences:\n",
    "        final_stop_sequences.append(item)\n",
    "final_stop_sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
